<div align="center">
  <picture>
      <img src="assets/header.png" width="70%" alt="CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning">
  </picture>
</div>

<hr>

<div align="center" style="line-height: 1;">
  <a href="https://github.com/deepreinforce-ai/CUDA-L1/blob/main/LICENSE"><img alt="License" src="https://img.shields.io/badge/License-GPL%20v3-blue.svg"/></a>
</div>


<p align="center">
<b>üè†&nbsp;&nbsp;<a href="https://deep-reinforce.com/">Project Page</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>üìÑ&nbsp;&nbsp;<a href="https://arxiv.org/abs/xxxx.xxxxx">Paper</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>üî•&nbsp;&nbsp;<a href="https://deepreinforce-ai.github.io/cudal1_blog/">Demo</a></b>
</p>

<div align="center">
  <picture>
      <img src="assets/ip_v2.png" style="max-height: 90px; width: auto;" alt="CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning">
  </picture>
</div>

## Introduction

In this paper, we introduce CUDA-L1, an automated reinforcement learning (RL) framework for CUDA optimization. The core of CUDA-L1 is a contrastive RL model, a newly-designed RL system to enhance optimization through comparative learning. 
CUDA-L1 achieves unprecedented performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of **√ó17.7** across all 250 CUDA kernels of KernelBench, with peak speedups reaching **√ó449**. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of **√ó17.8** on H100, **√ó19.0** on RTX 3090, **√ó16.5** on L40, **√ó14.7** on H800, and **√ó13.9** on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties:

- It discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance;
- It uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain "gatekeeper" techniques must be applied first to unlock the effectiveness of others;
- It identifies non-obvious performance bottlenecks (such as CPU-GPU synchronization dominating compute optimizations) and rejects seemingly beneficial optimizations that actually harm performance.
<div align="center">
  <picture>
      <img src="assets/general_result.png" width="90%" alt="Evaluation Results">
  </picture>
</div>


### How CUDA-L1 Works?

<div align="center">
  <picture>
      <img src="assets/pipeline.png" width="90%" alt="CUDA-L1 Pipeline">
  </picture>
</div>

#### Stage 1: Supervised Learning
We augment the training dataset with CUDA code variants generated by LLMs and fine-tune the base model on executable and correct implementations to establish foundational CUDA knowledge.

#### Stage 2: Self-Supervised Learning  
The model iteratively generates CUDA kernels, validates their correctness and executability, and trains on successfully validated examples, enabling autonomous improvement without human supervision.

#### Stage 3: Contrastive Reinforcement Learning
We employ contrastive learning with execution-time rewards, training the model to distinguish between faster and slower CUDA implementations, ultimately optimizing for superior performance.



## Evaluation Results

#### Performance on KernelBench

<div align="center">
<table>
<thead>
<tr>
<th align="center">Method</th>
<th align="center">Mean</th>
<th align="center">Max</th>
<th align="center">75%</th>
<th align="center">50%</th>
<th align="center">25%</th>
<th align="center">Success<br><sup># out of total</sup></th>
<th align="center">Speedup<br><sup># out of total</sup></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><strong>All Levels</strong></td>
<td align="center"><strong>17.7√ó</strong></td>
<td align="center"><strong>449√ó</strong></td>
<td align="center">7.08√ó</td>
<td align="center">1.81√ó</td>
<td align="center">1.22√ó</td>
<td align="center">249/250</td>
<td align="center">242/250</td>
</tr>
<tr>
<td align="center">Level 1</td>
<td align="center">12.3√ó</td>
<td align="center">166√ó</td>
<td align="center">9.28√ó</td>
<td align="center">1.65√ó</td>
<td align="center">1.15√ó</td>
<td align="center">99/100</td>
<td align="center">96/100</td>
</tr>
<tr>
<td align="center">Level 2</td>
<td align="center">6.39√ó</td>
<td align="center">111√ó</td>
<td align="center">4.42√ó</td>
<td align="center">1.61√ó</td>
<td align="center">1.24√ó</td>
<td align="center">100/100</td>
<td align="center">97/100</td>
</tr>
<tr>
<td align="center">Level 3</td>
<td align="center">50.8√ó</td>
<td align="center">449√ó</td>
<td align="center">22.9√ó</td>
<td align="center">2.66√ó</td>
<td align="center">1.58√ó</td>
<td align="center">50/50</td>
<td align="center">49/50</td>
</tr>
</tbody>
</table>
</div>

---

#### Cross-GPU Performance

<div align="center">
<table>
<thead>
<tr>
<th align="center">GPU Device</th>
<th align="center">Mean</th>
<th align="center">Max</th>
<th align="center">75%</th>
<th align="center">50%</th>
<th align="center">25%</th>
<th align="center">Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">A100 PCIe</td>
<td align="center">17.7√ó</td>
<td align="center">449√ó</td>
<td align="center">7.08√ó</td>
<td align="center">1.81√ó</td>
<td align="center">1.22√ó</td>
<td align="center">99.6%</td>
</tr>
<tr>
<td align="center">H100 XSM</td>
<td align="center">17.8√ó</td>
<td align="center">1,001√ó</td>
<td align="center">4.02√ó</td>
<td align="center">1.63√ó</td>
<td align="center">1.16√ó</td>
<td align="center">98.4%</td>
</tr>
<tr>
<td align="center">RTX 3090</td>
<td align="center">19.0√ó</td>
<td align="center">611√ó</td>
<td align="center">4.41√ó</td>
<td align="center">1.44√ó</td>
<td align="center">1.11√ó</td>
<td align="center">98.4%</td>
</tr>
<tr>
<td align="center">L40</td>
<td align="center">16.5√ó</td>
<td align="center">365√ó</td>
<td align="center">6.17√ó</td>
<td align="center">1.61√ó</td>
<td align="center">1.15√ó</td>
<td align="center">98.8%</td>
</tr>
<tr>
<td align="center">H800 XSM</td>
<td align="center">14.7√ó</td>
<td align="center">433√ó</td>
<td align="center">4.80√ó</td>
<td align="center">1.57√ó</td>
<td align="center">1.16√ó</td>
<td align="center">99.6%</td>
</tr>
<tr>
<td align="center">H20</td>
<td align="center">13.9√ó</td>
<td align="center">412√ó</td>
<td align="center">4.76√ó</td>
<td align="center">1.54√ó</td>
<td align="center">1.16√ó</td>
<td align="center">99.2%</td>
</tr>
</tbody>
</table>
</div>

<sup>
‚Ä¢ CUDA-L1 was trained on A100 GPUs but shows excellent transfer to other architectures
</sup><br/><sup>
‚Ä¢ Level 3 tasks (complex ML operations) show the highest speedups, making CUDA-L1 especially valuable for real-world applications
</sup>


#### Compare with Baseline Methods

We also compare baseline methods (backboned by Deepseek-R1, OpenAI O1) with CUDA-L1 on KernelBench. 


<div align="center">
  <picture>
      <img src="assets/general_result_plot_img.png" width="90%" alt="CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning">
  </picture>
</div>


## Want to reproduce our results?

We provide CUDA code snippets optimized by CUDA-L1 in the `optimized_cuda_code` folder, with separate versions for each GPU device. For example, to reproduce our results on H100 XSM, download `./optimized_cuda_code/h100_xsm.json` and run each code snippet on your H100 device.


## Citation
```latex
@article{deepreinforce2025cudal1,
  title={CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning},
  author={Li, Xiaoya and Sun, Xiaofei and Wang, Albert and Li, Jiwei and Chris, Shum},
  journal={arXiv preprint arXiv:2106.16038},
  year={2025}
}
```

## Contact
If you have any questions, please reach out to us at **research@deep-reinforce.com**.