{"info_string": "", "start_time": "2025-08-10 11:38:18", "code": "import torch\nimport torch.nn as nn\n\nclass ModelNew(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0):\n        \"\"\"\n        Initialize the LSTM model with optimized CUDA implementation.\n\n        :param input_size: The number of expected features in the input `x`\n        :param hidden_size: The number of features in the hidden state `h`\n        :param num_layers: Number of recurrent layers\n        :param output_size: The number of output features\n        :param dropout: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`\n        \"\"\"\n        super(ModelNew, self).__init__()\n        \n        # Register hidden states as buffers for automatic device management\n        self.register_buffer('h0', torch.randn((num_layers, batch_size, hidden_size)))\n        self.register_buffer('c0', torch.randn((num_layers, batch_size, hidden_size)))\n        \n        # Create the LSTM layer with optimal configuration\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout,\n            bidirectional=False\n        )\n        \n        # Linear layer for output\n        self.fc = nn.Linear(hidden_size, output_size)\n        \n        # Enable cuDNN benchmarking for optimal algorithm selection\n        if torch.cuda.is_available():\n            torch.backends.cudnn.benchmark = True\n        \n        # CUDA graph optimization\n        self.graph_cache = {}  # Cache for multiple input shapes\n        self.use_cuda_graph = torch.cuda.is_available()\n        self.warmup_count = 3  # Optimal warmup count based on previous experiments\n        \n    def _warmup(self, x):\n        \"\"\"Perform warmup iterations to stabilize performance\"\"\"\n        with torch.no_grad():\n            for _ in range(self.warmup_count):\n                out, _ = self.lstm(x, (self.h0, self.c0))\n                self.fc(out[:, -1, :])\n    \n    def _create_cuda_graph(self, x):\n        \"\"\"Create and capture CUDA graph for the forward pass\"\"\"\n        try:\n            # Create static inputs for CUDA graph capture\n            static_input = torch.zeros_like(x, device=x.device)\n            graph_output = torch.zeros((x.size(0), self.fc.out_features), device=x.device)\n            \n            # Perform warmup iterations\n            self._warmup(x)\n            \n            # Capture the graph\n            g = torch.cuda.CUDAGraph()\n            with torch.cuda.graph(g):\n                # Forward pass through LSTM\n                out, _ = self.lstm(static_input, (self.h0, self.c0))\n                # Extract last timestep and pass through linear layer\n                out = self.fc(out[:, -1, :])\n                graph_output.copy_(out)\n            \n            # Cache the graph and associated tensors\n            shape_key = (x.shape[0], x.shape[1], x.shape[2])\n            self.graph_cache[shape_key] = {\n                'graph': g,\n                'static_input': static_input,\n                'output': graph_output\n            }\n            \n            # Run the graph once to ensure everything is initialized\n            static_input.copy_(x, non_blocking=True)\n            g.replay()\n            return True\n        except Exception:\n            # Fallback to standard execution if graph capture fails\n            return False\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass through the LSTM model.\n\n        :param x: The input tensor, shape (batch_size, sequence_length, input_size)\n        :return: The output tensor, shape (batch_size, output_size)\n        \"\"\"\n        # Fast path: ensure tensors are on the correct device\n        device = self.h0.device\n        if x.device != device:\n            x = x.to(device, non_blocking=True)\n        \n        # Ensure input is contiguous for better memory access patterns\n        if not x.is_contiguous():\n            x = x.contiguous()\n        \n        # Fast path: Use CUDA graph if available\n        if self.use_cuda_graph and x.is_cuda:\n            shape_key = (x.shape[0], x.shape[1], x.shape[2])\n            \n            # Check if we have a cached graph for this input shape\n            if shape_key in self.graph_cache:\n                cached = self.graph_cache[shape_key]\n                cached['static_input'].copy_(x, non_blocking=True)\n                cached['graph'].replay()\n                return cached['output']\n            \n            # Create new graph for this input shape\n            if self._create_cuda_graph(x):\n                cached = self.graph_cache[shape_key]\n                return cached['output']\n        \n        # Fallback path: Standard forward pass if CUDA graph is not used\n        out, _ = self.lstm(x, (self.h0, self.c0))\n        out = self.fc(out[:, -1, :])\n        \n        return out\n\n# CRITICAL: Keep ALL hyperparameters EXACTLY as shown in the reference implementation\nbatch_size = 10\nsequence_length = 512\ninput_size = 128\nhidden_size = 256\nnum_layers = 6\noutput_size = 10\ndropout = 0.0\n\ndef get_inputs():\n    return [torch.randn(batch_size, sequence_length, input_size)]\n\ndef get_init_inputs():\n    return [input_size, hidden_size, num_layers, output_size, dropout]"}
{"info_string": "", "info": "stage1:Compile Success", "time": "2025-08-10 19:38:18", "error": false, "done": false}
{"info_string": "", "info": "stage3:Correctness Check Success", "time": "2025-08-10 19:38:20", "error": false, "done": false, "duration": 1.6107666492462158}
{"info_string": "", "info": "stage4:Performance Evaluation", "time": "2025-08-10 19:38:20", "error": false, "done": false}
{"info_string": "", "n_trial": 10, "trial": 0, "gpu_index": 0, "score": 3.2667442824156536, "time": "2025-08-10 19:38:20", "gpu_execution_time": 0.4456490898132324, "ave_gpu_execution_time": 0.044564908981323245, "done": false, "duration": 0.9399740695953369, "error": false}
{"info_string": "", "n_trial": 10, "trial": 1, "gpu_index": 0, "score": 3.2196577561096147, "time": "2025-08-10 19:38:21", "gpu_execution_time": 0.441934139251709, "ave_gpu_execution_time": 0.0441934139251709, "done": false, "duration": 0.9530601501464844, "error": false}
{"info_string": "", "n_trial": 10, "trial": 2, "gpu_index": 0, "score": 3.2179090648669275, "time": "2025-08-10 19:38:22", "gpu_execution_time": 0.4418020124435425, "ave_gpu_execution_time": 0.04418020124435425, "done": false, "duration": 0.9342861175537109, "error": false}
{"info_string": "", "n_trial": 10, "trial": 3, "gpu_index": 0, "score": 3.229524813779343, "time": "2025-08-10 19:38:23", "gpu_execution_time": 0.4426026515960693, "ave_gpu_execution_time": 0.04426026515960693, "done": false, "duration": 0.9334776401519775, "error": false}
{"info_string": "", "n_trial": 10, "trial": 4, "gpu_index": 0, "score": 3.2126837854491668, "time": "2025-08-10 19:38:24", "gpu_execution_time": 0.4414972162246704, "ave_gpu_execution_time": 0.04414972162246704, "done": false, "duration": 0.9318513870239258, "error": false}
